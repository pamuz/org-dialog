# -*- eval: (org-dialog-mode 1); -*-
#+TITLE: org-dialog
#+PROPERTY: header-args :tangle org-dialog.el :padline yes

#+DIALOG_MODEL: gpt-5.2-chat
#+DIALOG_ENDPOINT: https://pablo-ml1b1csr-eastus2.cognitiveservices.azure.com/openai/v1/chat/completions
#+DIALOG_API_KEY: INFERENCE_API_KEY
#+DIALOG_SYSTEM: You are co-authoring an org mode file with me. Please respond only with utf8 characters. No em-dashes. No sleek arrows. You always reply in a dry concise manner. As short as possible.

* Scaffolding
#+begin_src emacs-lisp
  ;;; org-dialog.el --- Introduces executable PROMPT org blocks in org-mode -*- lexical-binding: t; -*-
  ;; Author: Pablo
  ;; Version: 0.1.0
  ;; Package-Requires: ((emacs "27.1") (org "9.0"))

  ;;; Code:
  (require 'org)
  (require 'org-element)
  (require 'json)
  (require 'url)
#+end_src


* Introduction
Goal extend Emacs OrgMode with a new kind of block: PROMPT.

As the user is authoring an org text, they can put down a prompt block

#+begin_src org :tangle no
  ,#+TITLE: LLMs as co-author of literate programming documents

  Large language models make natural co-authors for literate
  programming, where explanation and code are interwoven into a single
  readable narrative. Literate programming demands not just correct code
  but clear reasoning - a task that plays to an LLM's strengths. By
  partnering with a human author, an LLM can draft prose, suggest code,
  and help maintain a coherent narrative, making it easier to produce
  documents that read as well as they run.

  ,#+begin_prompt
  What should be the TOC for this document?
  ,#+end_prompt
#+end_src

A prompt block is executed by placing point anywhere inside it and
pressing =C-c C-c=. The system takes the org buffer up to the end of
that PROMPT block, converts it into an LLM request, sends it, and
receives a response. The response is then inserted into an ASSISTANT
block immediately after the PROMPT block. The user can continue
editing the document using the response, or directly incorporate parts
of it into the text.

#+begin_src org :tangle no
  ,#+TITLE: LLMs as co-author of literate programming documents

  Large language models make natural co-authors for literate
  programming, where explanation and code are interwoven into a single
  readable narrative.

  ,#+begin_prompt
  What should be the TOC for this document?
  ,#+end_prompt

  ,#+begin_assistant
  * Introduction
  * What is Literate Programming
  * Why LLMs are Good Co-authors
  * Workflow and Tooling
  * Examples
  * Limitations and Caveats
  * Conclusion
  ,#+end_assistant
#+end_src


* Key functionality
** Detect if pointer is at a PROMPT block
#+begin_src emacs-lisp
(defun org-dialog--pointer-at-prompt-p ()
  "Return non-nil if point is anywhere inside a PROMPT block."
  (let ((el (org-element-context)))
    (while (and el (not (eq (org-element-type el) 'special-block)))
      (setq el (org-element-property :parent el)))
    (and el
         (string-equal
          (upcase (org-element-property :type el))
          "PROMPT"))))
#+end_src

#+begin_src emacs-lisp :tangle no :results output
  ;; Literate programming test of org-dialog--pointer-at-prompt-p
  (with-temp-buffer
    (org-mode)
    (insert "#+TITLE: A test org document\n")
    (insert "* Introduction\n")
    (princ (format "On top of PROMPT block? %s\n" (org-dialog--pointer-at-prompt-p)))
    (insert "#+begin_prompt\n")
    (insert "A request for an LLM\n")
    (insert "#+end_prompt\n")
    (goto-char (point-min))
    (re-search-forward "A request for an LLM")
    (princ (format "On top of PROMPT block? %s" (org-dialog--pointer-at-prompt-p))))
#+end_src

** Parse org buffer into internal represenation of user/assistant messages

LLM APIs expect a sequence of alternating system, user, and assistant
messages.

Assistant messages map directly to the contents of ASSISTANT blocks.

User messages are constructed from all text before and between
ASSISTANT blocks.

Dev insight: Sending the entire document as a single user message
would lose this structure. Keeping prior responses as assistant
messages informs the model of what it has already said, which affects
future behavior. An important consequence is that you can edit an
assistant response to reflect the style or content you would have
preferred. On the next inference, the model will continue as if it had
replied that way, allowing deliberate steering of tone and style.

** Collecting message contents
#+begin_src emacs-lisp
  (defun org-dialog--collect-messages ()
    "Return ordered list of (role . content) from current buffer.
  Everything up to and including each PROMPT block is a single user message.
  ASSISTANT blocks become assistant messages."
    (let ((elements (org-element-parse-buffer))
  	(messages '())
  	(pos (point-min)))
      (org-element-map elements 'special-block
        (lambda (el)
  	(let* ((type (upcase (org-element-property :type el)))
  	       (begin (org-element-property :begin el))
  	       (end (org-element-property :end el))
  	       (contents-begin (org-element-property :contents-begin el))
  	       (contents-end (org-element-property :contents-end el))
  	       (content-up-to-el (buffer-substring-no-properties
  				  pos begin))
  	       (content (buffer-substring-no-properties contents-begin contents-end)))
  	  (cond
  	   ((string= type "PROMPT")
  	    (push (cons "user"
  			(format "<doc>%s</doc><task>%s</task>"
  				content-up-to-el content))
  		  messages)
  	    (setq pos end))
  	   ((string= type "ASSISTANT")
  	    (push (cons "assistant" content) messages)
  	    (setq pos end))))))
      (when (< pos (point-max))
        (push (cons "user"
  		  (buffer-substring-no-properties pos (point-max)))
  	    messages))
      (nreverse messages)))
#+end_src

#+begin_src emacs-lisp :tangle no :wrap example :results value pp
  ;; Literate test for org-dialog--collect-messages
  ;; Build a small org buffer with prompt and assistant blocks
  ;; Then assert message ordering and roles

  (with-temp-buffer
    (org-mode)
    (insert "Intro text\n\n")
    (insert "#+begin_prompt\nAsk something\n#+end_prompt\n\n")
    (insert "#+begin_assistant\nAnswer one\n#+end_assistant\n\n")
    (insert "More text\n")
    (insert "#+begin_prompt\nAsk something else\n#+end_prompt\n\n")
    (forward-line -1)
    (org-dialog--collect-messages))
#+end_src

#+RESULTS:
#+begin_example
(("user" . "<doc>Intro text\n\n</doc><task>Ask something\n</task>")
 ("assistant" . "Answer one\n")
 ("user" . "<doc>More text\n</doc><task>Ask something else\n</task>"))
#+end_example

** Read config from buffer keywords
Obtain the necessary LLM request parameters from buffer level keywords.  
These include model, endpoint, api key, and optional system prompt.  
They are read via `org-collect-keywords` before issuing the network request.

#+begin_src org :tangle no
  ,#+DIALOG_MODEL: gpt-5.2-chat
  ,#+DIALOG_ENDPOINT: https://example.openai.azure.com/openai/v1/chat/completions
  ,#+DIALOG_API_KEY: INFERENCE_API_KEY
  ,#+DIALOG_SYSTEM: You are a concise technical co-author.
#+end_src

#+begin_src emacs-lisp
  (defun org-dialog--collect-keywords ()
    (let ((kws (org-collect-keywords
  	      '("DIALOG_MODEL" "DIALOG_ENDPOINT" "DIALOG_API_KEY" "DIALOG_SYSTEM"))))
      (list :model (cadr (assoc "DIALOG_MODEL" kws))
  	  :endpoint (cadr (assoc "DIALOG_ENDPOINT" kws))
  	  :api-key (and (cadr (assoc "DIALOG_API_KEY" kws))
  			(getenv (cadr (assoc "DIALOG_API_KEY" kws))))
  	  :system (cadr (assoc "DIALOG_SYSTEM" kws)))))
#+end_src

#+begin_src emacs-lisp :tangle no :wrap example :results value pp
;; Literate test for org-dialog--collect-keywords
;; Verify buffer keywords are read and mapped correctly

(with-temp-buffer
  (org-mode)
  (insert "#+DIALOG_MODEL: test-model\n")
  (insert "#+DIALOG_ENDPOINT: https://example.test/chat\n")
  (insert "#+DIALOG_API_KEY: TEST_ENV_KEY\n")
  (insert "#+DIALOG_SYSTEM: Test system prompt\n")
  (setenv "TEST_ENV_KEY" "secret123")
  (org-dialog--collect-keywords))
#+end_src

#+RESULTS:
#+begin_example
(:model "test-model" :endpoint "https://example.test/chat" :api-key
	"secret123" :system "Test system prompt")
#+end_example

** Prepare LLM request
Create a request compatible with https://platform.openai.com/docs/api-reference/chat/create

#+begin_src emacs-lisp
  (defun org-dialog--prepare-request (messages config)
    "Build a complete LLM request from MESSAGES and CONFIG.
  MESSAGES is the output of `org-dialog--collect-messages'.
  CONFIG is the output of `org-dialog--config'.
  Returns plist (:endpoint :headers :payload)."
    (let* ((system (plist-get config :system))
  	 (api-messages
  	  (vconcat
  	   (when system
  	     (vector `((role . "developer") (content . ,system))))
  	   (mapcar (lambda (m)
  		     `((role . ,(car m)) (content . ,(cdr m))))
  		   messages))))
      (list
       :endpoint (plist-get config :endpoint)
       :headers `(("Content-Type" . "application/json")
  		("Authorization" . ,(concat "Bearer " (plist-get config :api-key))))
       :payload (encode-coding-string
  	       (json-serialize
  		`((model . ,(plist-get config :model))
  		  (messages . ,api-messages)))
  	       'utf-8))))
#+end_src

#+begin_src emacs-lisp :tangle no :wrap example :results value pp
;; Literate test for org-dialog--prepare-request

(let* ((messages '(("user" . "Hello") ("assistant" . "Hi")))
       (config '(:model "test-model"
                :endpoint "https://example.test/chat"
                :api-key "secret"
                :system "System prompt")))
       (org-dialog--prepare-request messages config)))
#+end_src

#+RESULTS:
#+begin_example
(:endpoint "https://example.test/chat"
 :headers (("Content-Type" . "application/json")
	    ("Authorization" . "Bearer secret"))
 :payload "{\"model\":\"test-model\",\"messages\":[{\"role\":\"developer\",\"content\":\"System prompt\"},{\"role\":\"user\",\"content\":\"Hello\"},{\"role\":\"assistant\",\"content\":\"Hi\"}]}")
#+end_example

** Execute LLM API reqeuest
We use =plz= package for this.

#+begin_src emacs-lisp
  (defun org-dialog--execute-request (request callback)
    "Execute LLM REQUEST and call CALLBACK with response string."
    (require 'plz)
    (plz 'post
      (plist-get request :endpoint)
      :headers (plist-get request :headers)
      :body (plist-get request :payload)
      :as 'string
      :then (lambda (resp)
              (funcall callback resp))
      :else (lambda (err)
              (message "org-dialog request failed: %s" err))))
#+end_src

#+begin_prompt
write a literate programming style test, I think you will need to use
the actual values from my keywords since this will hit a real endpoint
#+end_prompt

#+begin_src emacs-lisp :tangle no :wrap example 
;; Literate integration test for full org-dialog request cycle.
;; This test uses real buffer keywords and hits the configured endpoint.
;; It is not suitable for automated CI.

(with-temp-buffer
  (org-mode)
  ;; buffer level configuration
  (insert "#+DIALOG_MODEL: gpt-5.2-chat\n")
  (insert "#+DIALOG_ENDPOINT: https://pablo-ml1b1csr-eastus2.cognitiveservices.azure.com/openai/v1/chat/completions\n")
  (insert "#+DIALOG_API_KEY: INFERENCE_API_KEY\n")
  (insert "#+DIALOG_SYSTEM: You are a dry concise test assistant.\n\n")

  ;; minimal document with a prompt
  (insert "* Test\n")
  (insert "This is a test document.\n\n")
  (insert "#+begin_prompt\n")
  (insert "Reply with the word OK.\n")
  (insert "#+end_prompt\n")

  ;; collect inputs
  (let* ((messages (org-dialog--collect-messages))
         (config (org-dialog--collect-keywords))
         (request (org-dialog--prepare-request messages config))
         (done nil))
    (org-dialog--execute-request
     request
     (lambda (resp)
       (setq done resp)))
    ;; naive wait for async completion
    (while (not done)
      (sleep-for 1))
    done))
#+end_src

#+RESULTS:
#+begin_example
{
  "choices": [
    {
      "content_filter_results": {
        "hate": {
          "filtered": false,
          "severity": "safe"
        },
        "protected_material_code": {
          "filtered": false,
          "detected": false
        },
        "protected_material_text": {
          "filtered": false,
          "detected": false
        },
        "self_harm": {
          "filtered": false,
          "severity": "safe"
        },
        "sexual": {
          "filtered": false,
          "severity": "safe"
        },
        "violence": {
          "filtered": false,
          "severity": "safe"
        }
      },
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "annotations": [],
        "content": "OK",
        "refusal": null,
        "role": "assistant"
      }
    }
  ],
  "created": 1770581162,
  "id": "chatcmpl-D75KcqWoYy6C49fmuK8pjjEMEQGmo",
  "model": "gpt-5.2-chat-2025-12-11",
  "object": "chat.completion",
  "prompt_filter_results": [
    {
      "prompt_index": 0,
      "content_filter_results": {
        "hate": {
          "filtered": false,
          "severity": "safe"
        },
        "jailbreak": {
          "filtered": false,
          "detected": false
        },
        "self_harm": {
          "filtered": false,
          "severity": "safe"
        },
        "sexual": {
          "filtered": false,
          "severity": "safe"
        },
        "violence": {
          "filtered": false,
          "severity": "safe"
        }
      }
    }
  ],
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 11,
    "completion_tokens_details": {
      "accepted_prediction_tokens": 0,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": 0
    },
    "prompt_tokens": 114,
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 0
    },
    "total_tokens": 125
  }
}
#+end_example

** Inserting content wrapped in an ASSISTANT block right after a PROMPT block
#+begin_src emacs-lisp
  (defun org-dialog--insert-assistant-after-prompt (buffer prompt-end content)
    "Insert or replace ASSISTANT block after PROMPT in BUFFER."
    (with-current-buffer buffer
      (let ((end (copy-marker prompt-end)))
        (save-excursion
  	(goto-char end)
  	(skip-chars-forward " \t\r\n")
  	(let ((next (org-element-at-point)))
  	  (when (and (eq (org-element-type next) 'special-block)
  		     (string= (upcase (org-element-property :type next)) "ASSISTANT"))
  	    (delete-region (org-element-property :begin next)
  			   (org-element-property :end next))
  	    (delete-region end (progn (goto-char end)
  				      (skip-chars-forward " \t\r\n")
  				      (point)))))
  	(goto-char end)
  	(insert "\n\n#+begin_assistant\n"
  		content
  		(unless (string-suffix-p "\n" content) "\n")
  		"#+end_assistant")))))
#+end_src

#+begin_src emacs-lisp :tangle no :wrap example
;; Literate test for org-dialog--insert-assistant-after-prompt.
;; Verifies insert and replace behavior, independent of point.

(with-temp-buffer
  (org-mode)
  (insert "* Test\n\n")
  (insert "#+begin_prompt\nPrompt text\n#+end_prompt\n")
  (let ((buf (current-buffer))
        (prompt-end (save-excursion
                      (goto-char (point-min))
                      (search-forward "#+end_prompt")
                      (line-end-position))))
    ;; first insertion
    (org-dialog--insert-assistant-after-prompt
     buf prompt-end "First response")
    ;; replacement
    (org-dialog--insert-assistant-after-prompt
     buf prompt-end "Second response")
    (buffer-string)))
#+end_src

#+RESULTS:
#+begin_example
,* Test

,#+begin_prompt
Prompt text
,#+end_prompt

,#+begin_assistant
Second response
,#+end_assistant
#+end_example

#+begin_prompt
we should always leave one line of separation between prompt and assistant blocks
#+end_prompt

#+begin_assistant
Yes. Insert an extra newline before the ASSISTANT block.

Change insert to:

,#+begin_src emacs-lisp
(insert "\n\n#+begin_assistant\n"
        content
        (unless (string-suffix-p "\n" content) "\n")
        "#+end_assistant")
,#+end_src
#+end_assistant

#+begin_prompt
wont that affect when there are already several newlines after prompt block?
#+end_prompt

#+begin_assistant
Yes. Better normalize spacing.

After deleting any existing ASSISTANT, do:

,#+begin_src emacs-lisp
(delete-region end (progn (goto-char end)
                          (skip-chars-forward " \t\r\n")
                          (point)))
(insert "\n\n#+begin_assistant\n"
        content
        (unless (string-suffix-p "\n" content) "\n")
        "#+end_assistant")
,#+end_src
#+end_assistant

** Executing a PROMPT block
#+begin_src emacs-lisp
(defun org-dialog-execute-prompt ()
  "Execute PROMPT block at point and insert ASSISTANT response."
  (interactive)
  (unless (org-dialog--pointer-at-prompt-p)
    (user-error "Not on a PROMPT block"))
  (let* ((el (org-element-at-point))
         (prompt-end
          (save-excursion
            (goto-char (org-element-property :end el))
            (forward-line 1)
            (point)))
         (buffer (current-buffer))
         (messages (org-dialog--collect-messages))
         (config (org-dialog--collect-keywords))
         (request (org-dialog--prepare-request messages config)))
    (org-dialog--execute-request
     request
     (lambda (resp)
       (let* ((json (json-parse-string resp :object-type 'alist))
              (choices (alist-get 'choices json))
              (msg (alist-get 'message (aref choices 0)))
              (content (alist-get 'content msg)))
         (org-dialog--insert-assistant-after-prompt
          buffer prompt-end content))))))
#+end_src

#+begin_src emacs-lisp :tangle no :wrap example
;; Literate integration test for org-dialog-execute-prompt.
;; Requires real buffer keywords and a live endpoint.
;; Execute with point inside the PROMPT block.

(with-temp-buffer
  (org-mode)
  ;; config
  (insert "#+DIALOG_MODEL: gpt-5.2-chat\n")
  (insert "#+DIALOG_ENDPOINT: https://pablo-ml1b1csr-eastus2.cognitiveservices.azure.com/openai/v1/chat/completions\n")
  (insert "#+DIALOG_API_KEY: INFERENCE_API_KEY\n")
  (insert "#+DIALOG_SYSTEM: You are a dry concise test assistant.\n\n")

  ;; document
  (insert "* Test\n")
  (insert "Prelude text.\n\n")
  (insert "#+begin_prompt\n")
  (insert "Reply with OK.\n")
  (insert "#+end_prompt\n")

  ;; place point inside prompt
  (goto-char (point-min))
  (search-forward "Reply with OK")

  ;; execute
  (org-dialog-execute-prompt)

  ;; wait for async completion
  (while (not (save-excursion
                (goto-char (point-min))
                (search-forward "#+begin_assistant" nil t)))
    (sleep-for 1))

  ;; return buffer for inspection
  (buffer-string))
#+end_src

#+RESULTS:
#+begin_example
,#+DIALOG_MODEL: gpt-5.2-chat
,#+DIALOG_ENDPOINT: https://pablo-ml1b1csr-eastus2.cognitiveservices.azure.com/openai/v1/chat/completions
,#+DIALOG_API_KEY: INFERENCE_API_KEY
,#+DIALOG_SYSTEM: You are a dry concise test assistant.

,* Test
Prelude text.

,#+begin_prompt
Reply with OK.
,#+end_prompt

,#+begin_assistant
OK
,#+end_assistant
#+end_example

** Minor mode
#+begin_src emacs-lisp
(defvar org-dialog-mode-map
  (let ((map (make-sparse-keymap)))
    (define-key map (kbd "C-c C-c") #'org-dialog--maybe-execute)
    map))

(defun org-dialog--maybe-execute ()
  (interactive)
  (if (org-dialog--pointer-at-prompt-p)
      (org-dialog-execute-prompt)
    (org-ctrl-c-ctrl-c)))

(define-minor-mode org-dialog-mode
  "Execute PROMPT blocks with C-c C-c."
  :lighter " Dialog"
  :keymap org-dialog-mode-map)

(provide 'org-dialog)
#+end_src

** Next steps
#+begin_prompt
what's next, how can we improve?
#+end_prompt


#+begin_assistant
Concise roadmap:

- Stream responses and insert incrementally.
- Cursor and window preservation.
- Prompt scoping. Execute up to point, not full buffer.
- Error surfacing in an ASSISTANT or ERROR block.
- Model params keywords: temperature, max tokens.
- Async cancel and retry.
- History trimming for long docs.
- Tests for nested blocks and edits.
- Documentation and example workflows.
#+end_assistant
